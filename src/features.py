# -*- coding: utf-8 -*-
"""feature Engineering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VdGWsbstpUCkrkdAjRU7VTQqApnEVe3V
"""

# !pip install transformers

import pandas as pd
from pathlib import Path
import numpy as np
import json
import torch.nn.functional as F
import torch
from transformers import BertModel, BertTokenizerFast
import pickle
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import gc
from tqdm import tqdm
import random
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# from google.colab import drive
# drive.mount('/content/drive')

# path = "/content/drive/My Drive/CS224N/final_phase2/"

def get_label(row):
  label = row["type"]
  label_idx = label_map[label]
  return float(label_idx)

data = pd.read_csv("../Datasets/raw/mbti_tweets.csv")
data = data[(data["type"] == "INFP") | (data["type"] == "INTP")]
data = data.reset_index()
label_map = {label: i for i, label in enumerate(set(data["type"]))}
data["type"] = data.apply(get_label,axis=1)

import pandas as pd
from pathlib import Path
import numpy as np
import json
import torch.nn.functional as F
import torch
from transformers import BertModel, BertTokenizerFast
import pickle


max_word_length = 768


def sentence_length(row):
    return torch.tensor(len(row["posts"])).float()


def word_length(row):
    word_length_seq = np.array([len(i) for i in row["posts"].replace("|||", ' ').split(' ')][:max_word_length])
    if len(word_length_seq) != max_word_length:
        word_length_seq = np.pad(word_length_seq, (0, max_word_length - len(word_length_seq)), 'constant', constant_values=0)
    return torch.tensor(word_length_seq).float()


def create_word2ind(row):
  count = 1
  word2ind = {"UNK" : 0}
  ind2word = {0 : "UNK"}
  words = row["posts"].replace("|||", ' ').split(' ')
  for word in words:
    if word not in word2ind:
      word2ind[word] = count
      ind2word[count] = word
      count += 1
  with open(f'../experiments/word2ind.pickle', 'wb') as handle:
    pickle.dump(word2ind, handle)
  with open(f'../experiments/ind2word.pickle', 'wb') as handle:
    pickle.dump(ind2word, handle)

def get_word_index(row):
  with open(f'../experiments/word2ind.pickle', 'rb') as f:
    word2ind = pickle.load(f)
  indexes = []
  words = row["posts"].replace("|||", ' ').split(' ')
  for word in words:
    ind = "0"
    if word in word2ind:
      ind = word2ind[word]
    indexes.append(int(ind))
    if len(indexes) == max_word_length:
      break
  if len(indexes) != max_word_length:
    for i in range(max_word_length - len(indexes)):
            indexes.append(0)
  return torch.tensor(np.array(indexes)).float()


def get_word_index_bigram(row):
  with open(f'../experiments/word2ind.pickle', 'rb') as f:
    word2ind = pickle.load(f)
  indexes = []
  words = row["posts"].replace("|||", ' ').split(' ')
  ind_curr = "0"
  for i in range(len(words)):
    ind_pre = ind_curr
    if words[i] in word2ind:
      ind_curr = word2ind[words[i]]
    else:
      ind_curr = 0
    indexes.append(int(f"{ind_pre}{ind_curr}"))
    if len(indexes) == max_word_length:
      break
  if len(indexes) != max_word_length:
    for i in range(max_word_length - len(indexes)):
            indexes.append(0)
  return torch.tensor(np.array(indexes)).float()


def get_word_to_vec(row):
    with open(f'../models/all.word2vev.pickle', 'rb') as f:
      word2vec = pickle.load(f)

    with open(f'../models/all.tokens.pickle', 'rb') as f:
      tokens = pickle.load(f)

    words = row["posts"].replace("|||", ' ').split(' ')

    vectors = []
    for word in words:
      vector = np.zeros(10)
      if word in tokens:
        vector = word2vec[tokens[word]]
      vectors.append(vector)

    if len(vectors) < max_word_length:
      for i in range(max_word_length - len(vectors)):
        vectors.append(np.zeros(10))
    return torch.tensor(np.array(vectors[:max_word_length])).float()



def get_word_to_vec_bi(row):
    with open(f'../models/all.word2vev.pickle', 'rb') as f:
      word2vec = pickle.load(f)

    with open(f'../models/all.tokens.pickle', 'rb') as f:
      tokens = pickle.load(f)

    words = row["posts"].replace("|||", ' ').split(' ')

    vectors = []
    current_vec = np.zeros(10)
    for word in words:
      per_vec = current_vec
      if word in tokens:
        current_vec = word2vec[tokens[word]]
      else:
        current_vec = np.zeros(10)
      vectors.append(np.concatenate([per_vec, current_vec]))

    if len(vectors) < max_word_length:
      for i in range(max_word_length - len(vectors)):
        vectors.append(np.zeros(20))
    return torch.tensor(vectors[:max_word_length]).float()

def get_embedding(post, tokenizer, embeddings_dict):
    tweets = post.split("|||")[:50]
    while len(tweets) < 50:
        tweets.append("empty tweet")
    tokenized_input = tokenizer.batch_encode_plus(tweets, add_special_tokens=True, truncation=True,
                                                  padding=True)
    input_ids_batch = torch.tensor(tokenized_input["input_ids"])
    embeddings = embeddings_dict[input_ids_batch]
    embedding_avg = torch.sum(embeddings, axis=1) / embeddings.size()[1]
    final_embedding = torch.sum(embedding_avg, axis=0)

    return torch.tensor(final_embedding)


def get_bert_embeddings(data):
    tokenizer = BertTokenizerFast.from_pretrained("bert-base-cased")
    bert = BertModel.from_pretrained("bert-base-cased")
    embedding_matrix = bert.embeddings.word_embeddings.weight
    return torch.stack(data["posts"].apply(lambda x: get_embedding(x, tokenizer, embedding_matrix)).tolist())

bert_embeddings = get_bert_embeddings(data).detach().float()

# data["sentence_len"] = data.apply(sentence_length, axis=1)
data["word_len"] = data.apply(word_length, axis=1)
data.apply(create_word2ind, axis=1)
data["word2ind"] = data.apply(get_word_index, axis=1)
data["word2ind_bi"] = data.apply(get_word_index_bigram, axis=1)
data["word2vec"] = data.apply(get_word_to_vec, axis=1)
data["word2vec_bi"] = data.apply(get_word_to_vec_bi, axis=1)

class MyDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels
        # self.num_classes = 16
        # self.label_map = label_map

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        sample = self.data[index]
        label = self.labels[index]
        # label_idx = self.label_map[label]
        # one_hot_label = F.one_hot(torch.tensor(label_idx), num_classes=self.num_classes).float()
        return sample, label

all_acc = {}
all_loss = {}

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import torch.nn.functional as F
import gc
from tqdm import tqdm
from pathlib import Path
import random



class MBTI_model(nn.Module):
    def __init__(self, input_dim):
        super(MBTI_model, self).__init__()
        self.fc_1 = nn.Linear(input_dim, 16)
        self.fc_2 = nn.Linear(16, 1)
        # self.fc_3 = nn.Linear(128, 64)
        # self.fc_4 = nn.Linear(64, 16)
        self.flatten = nn.Flatten()

    def forward(self, x):
        x = self.flatten(x)
        x = self.fc_1(x)
        x = F.relu(x)
        x = self.fc_2(x)
        # x = self.dropout(x)
        # x = F.relu(x)
        # x = self.fc_3(x)
        # x = self.dropout(x)
        # x = F.relu(x)
        # x = self.fc_4(x)
        x = F.sigmoid(x)
        x = x.squeeze(0)
        return x.float()

def train(model, train_dataloader, val_dataloader, test_dataloader, num_epochs=10):
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    epochs_loss = {"train": [], "val": [], "test": []}
    epochs_acc = {"train": [], "val": [], "test": []}

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct_predictions = 0
        total_predictions = 0
        for inputs, labels in train_dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()


            outputs = model(inputs)
            loss = criterion(outputs.float(), labels.float())
            loss.backward()
            optimizer.step()

            _, predicted = torch.max(outputs, 0)
            total_predictions += labels.size(0)
            correct_predictions += (predicted == labels).sum().item()

            running_loss += loss.item()
            optimizer.zero_grad()
            outputs.detach()

        train_loss = running_loss / len(train_dataloader)
        train_accuracy = correct_predictions / total_predictions

        model.eval()
        val_running_loss = 0.0
        val_correct_predictions = 0
        val_total_predictions = 0

        with torch.no_grad():
            for val_inputs, val_labels in val_dataloader:
                val_inputs = val_inputs.to(device)
                val_labels = val_labels.to(device)

                val_outputs = model(val_inputs)

                loss = criterion(val_outputs.float(), val_labels.float())

                _, val_predicted = torch.max(val_outputs, 0)
                val_total_predictions += val_labels.size(0)
                val_correct_predictions += (val_predicted == val_labels).sum().item()
                val_running_loss += loss.item()

        val_loss = val_running_loss / len(val_dataloader)
        val_accuracy = val_correct_predictions / val_total_predictions

        epochs_loss["train"].append(train_loss)
        epochs_acc["train"].append(train_accuracy)
        epochs_loss["val"].append(val_loss)
        epochs_acc["val"].append(val_accuracy)

        print(f"Epoch [{epoch+1}/{num_epochs}]: \n"
              f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} \n"
              f"Val Loss: {train_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")
        print("===========================================================================")

    #     if val_accuracy > best_val_accuracy:
    #         best_val_accuracy = val_accuracy
    #         best_model_weights = model.state_dict()

    # model.load_state_dict(best_model_weights)

    model.eval()
    test_running_loss = 0.0
    test_correct_predictions = 0
    test_total_predictions = 0

    with torch.no_grad():
        for test_inputs, test_labels in test_dataloader:
            test_inputs = test_inputs.to(device)
            test_labels = test_labels.to(device)

            test_outputs = model(test_inputs)

            loss = criterion(test_outputs.float(), test_labels.float())

            _, test_predicted = torch.max(test_outputs, 0)
            test_total_predictions += test_labels.size(0)
            test_correct_predictions += (test_predicted == test_labels).sum().item()
            test_running_loss += loss.item()

        test_accuracy = test_correct_predictions / test_total_predictions
        test_loss = test_running_loss / len(test_dataloader)

        epochs_loss["test"].append(test_loss)
        epochs_acc["test"].append(test_accuracy)

        print(f"Training completed! Test Accuracy: {test_accuracy:.4f}")

        # torch.save(model.state_dict(), path + f"models/conv_model.pth")


    return epochs_loss, epochs_acc

train_data = data.iloc[:2000].reset_index()
val_data = data.iloc[2000:2500].reset_index()
test_data = data.iloc[2500:].reset_index()

sentence_len_train_data = MyDataset(train_data["sentence_len"], train_data["type"])
sentence_len_val_data = MyDataset(val_data["sentence_len"], val_data["type"])
sentence_len_test_data = MyDataset(test_data["sentence_len"], test_data["type"])

batch_size = 1
sentence_len_train_dataloader = DataLoader(sentence_len_train_data, batch_size=batch_size, shuffle=True)
sentence_len_val_dataloader = DataLoader(sentence_len_val_data, batch_size=batch_size, shuffle=True)
sentence_len_test_dataloader = DataLoader(sentence_len_test_data, batch_size=batch_size, shuffle=True)

model = MBTI_model(input_dim=1)
epochs_loss, epochs_acc = train(model, sentence_len_train_dataloader, sentence_len_val_dataloader, sentence_len_test_dataloader)
all_acc["sentence_len"] = epochs_acc
all_loss["sentence_len"] = epochs_loss

word_len_train_data = MyDataset(train_data["word_len"], train_data["type"])
word_len_val_data = MyDataset(val_data["word_len"], val_data["type"])
word_len_test_data = MyDataset(test_data["word_len"], test_data["type"])

batch_size = 1
word_len_train_dataloader = DataLoader(word_len_train_data, batch_size=batch_size, shuffle=True)
word_len_val_dataloader = DataLoader(word_len_val_data, batch_size=batch_size, shuffle=True)
word_len_test_dataloader = DataLoader(word_len_test_data, batch_size=batch_size, shuffle=True)

model = MBTI_model(input_dim=768)
epochs_loss, epochs_acc = train(model, word_len_train_dataloader, word_len_val_dataloader, word_len_test_dataloader)
all_acc["word_len"] = epochs_acc
all_loss["word_len"] = epochs_loss

word2ind_train_data = MyDataset(train_data["word2ind"], train_data["type"])
word2ind_val_data = MyDataset(val_data["word2ind"], val_data["type"])
word2ind_test_data = MyDataset(test_data["word2ind"], test_data["type"])

batch_size = 1
word2ind_train_dataloader = DataLoader(word2ind_train_data, batch_size=batch_size, shuffle=True)
word2ind_val_dataloader = DataLoader(word2ind_val_data, batch_size=batch_size, shuffle=True)
word2ind_test_dataloader = DataLoader(word2ind_test_data, batch_size=batch_size, shuffle=True)

model = MBTI_model(input_dim=768)
epochs_loss, epochs_acc = train(model, word2ind_train_dataloader, word2ind_val_dataloader, word2ind_test_dataloader)
all_acc["word2ind"] = epochs_acc
all_loss["word2ind"] = epochs_loss

word2ind_bi_train_data = MyDataset(train_data["word2ind_bi"], train_data["type"])
word2ind_bi_val_data = MyDataset(val_data["word2ind_bi"], val_data["type"])
word2ind_bi_test_data = MyDataset(test_data["word2ind_bi"], test_data["type"])

batch_size = 1
word2ind_bi_train_dataloader = DataLoader(word2ind_bi_train_data, batch_size=batch_size, shuffle=True)
word2ind_bi_val_dataloader = DataLoader(word2ind_bi_val_data, batch_size=batch_size, shuffle=True)
word2ind_bi_test_dataloader = DataLoader(word2ind_bi_test_data, batch_size=batch_size, shuffle=True)

model = MBTI_model(input_dim=768)
epochs_loss, epochs_acc = train(model, word2ind_bi_train_dataloader, word2ind_bi_val_dataloader, word2ind_bi_test_dataloader)
all_acc["word2ind_bi"] = epochs_acc
all_loss["word2ind_bi"] = epochs_loss

word2vec_train_data = MyDataset(train_data["word2vec"], train_data["type"])
word2vec_val_data = MyDataset(val_data["word2vec"], val_data["type"])
word2vec_test_data = MyDataset(test_data["word2vec"], test_data["type"])

batch_size = 1
word2vec_train_dataloader = DataLoader(word2vec_train_data, batch_size=batch_size, shuffle=True)
word2vec_val_dataloader = DataLoader(word2vec_val_data, batch_size=batch_size, shuffle=True)
word2vec_test_dataloader = DataLoader(word2vec_test_data, batch_size=batch_size, shuffle=True)

model = MBTI_model(input_dim=7680)
epochs_loss, epochs_acc = train(model, word2vec_train_dataloader, word2vec_val_dataloader, word2vec_test_dataloader)
all_acc["word2vec"] = epochs_acc
all_loss["word2vec"] = epochs_loss

word2vec_bi_train_data = MyDataset(train_data["word2vec_bi"], train_data["type"])
word2vec_bi_val_data = MyDataset(val_data["word2vec_bi"], val_data["type"])
word2vec_bi_test_data = MyDataset(test_data["word2vec_bi"], test_data["type"])

batch_size = 1
word2vec_bi_train_dataloader = DataLoader(word2vec_bi_train_data, batch_size=batch_size, shuffle=True)
word2vec_bi_val_dataloader = DataLoader(word2vec_bi_val_data, batch_size=batch_size, shuffle=True)
word2vec_bi_test_dataloader = DataLoader(word2vec_bi_test_data, batch_size=batch_size, shuffle=True)

model = MBTI_model(input_dim=768*20)
epochs_loss, epochs_acc = train(model, word2vec_bi_train_dataloader, word2vec_bi_val_dataloader, word2vec_bi_test_dataloader)
all_acc["word2vec_bi"] = epochs_acc
all_loss["word2vec_bi"] = epochs_loss

bert_embeddings_train_data = MyDataset(bert_embeddings[:2000], train_data["type"])
bert_embeddings_val_data = MyDataset(bert_embeddings[2000:2500], val_data["type"])
bert_embeddings_test_data = MyDataset(bert_embeddings[2500:], test_data["type"])

batch_size = 1
bert_embeddings_train_dataloader = DataLoader(bert_embeddings_train_data, batch_size=batch_size, shuffle=True)
bert_embeddings_val_dataloader = DataLoader(bert_embeddings_val_data, batch_size=batch_size, shuffle=True)
bert_embeddings_test_dataloader = DataLoader(bert_embeddings_test_data, batch_size=batch_size, shuffle=True)

model = MBTI_model(input_dim=768)
epochs_loss, epochs_acc = train(model, bert_embeddings_train_dataloader, bert_embeddings_val_dataloader, bert_embeddings_test_dataloader)
all_acc["bert_embeddings"] = epochs_acc
all_loss["bert_embeddings"] = epochs_loss

def draw_plot(mydict, name, validation_name):
  for k, v in mydict.items():
    plt.plot(v, label=f"{name}->{k}")
    plt.xlabel("epochs")
    plt.ylabel(validation_name)
    plt.legend()

for k, v in all_acc.items():
  draw_plot(all_acc[k], k, "accuracy")
  plt.savefig(f"../stats/acc_plot_{k}.png")
  plt.figure()

for k, v in all_loss.items():
  draw_plot(all_loss[k], k, "loss")
  plt.savefig(f"../stats/loss_plot_{k}.png")
  plt.figure()

